{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for link prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we create the dataset required for our link prediction task. Note, however, that this dataset is only used for our GNN applications. \n",
    "For the baselines, we only load the 5821 links of our graph and use them to create a networkx or networkit graph that we can use to make all the predictions that we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.transforms import NormalizeFeatures, RandomLinkSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(tuples):\n",
    "    \"\"\"\n",
    "    Reverse a 2-tuple.\n",
    "    \"\"\"\n",
    "    new_tup = tuples[::-1]\n",
    "    \n",
    "    return new_tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction\n",
    "\n",
    "Link prediction is about predicting links between pairs of nodes. In our task, we focus on the methods summarized in <span style=\"font-variant:small-caps;\">Wu et al. (2022)</span> who heavily based their work on link prediction on <span style=\"font-variant:small-caps;\">Zhang and Chen (2018)</span> and <span style=\"font-variant:small-caps;\">Kipf and Welling (2016)</span>.\n",
    "\n",
    "In our dataset, we use all 5821 links as well as all 68 available node features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Geometric functionality for dataset creation\n",
    "\n",
    "We use the built-in dataset creation provided in PyTorch Geometric. For more information on dataset creation in PyTorch Geometric, click [here](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_dataset.html).\n",
    "\n",
    "PyTorch Geometric provides two abstract classes for datasets: [`torch_geometric.data.Dataset`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Dataset) and [`torch_geometric.data.InMemoryDataset`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.InMemoryDataset). Note that `torch_geometric.data.InMemoryDataset` inherits from `torch_geometric.data.Dataset` and should be used if the whole dataset fits into CPU memory.\n",
    "\n",
    "Each dataset is passed a root folder, indicating where the datasets should be stored. This root folder is split up into two folders: the `raw_dir`, where the dataset is downloaded to, and the `processed_dir`, where the processed dataset is saved.\n",
    "\n",
    "Additionally, each dataset can be passed a `transform`, a `pre_transform` and a `pre_filter` function, all being `None` by default. The `transform` function dynamically transforms the data object before accessing, therefore being particularly useful for data augmentation. The `pre_transform` function applies the transformation before saving the data objects to disk, therefore being particularly useful for heavy precomputation which needs to be done only once. The `pre_filter` function can manually filter out data objects before saving, therefore being particularly useful for use cases which involve the restriction of data objects being of a specific class.\n",
    "\n",
    "To create a `torch_geometric.data.InMemoryDataset`, we need to implement four fundamental methods:\n",
    "* `torch_geometric.data.Dataset.raw_file_names()`: A list of files in the `raw_dir` which needs to be found in order to skip the download.\n",
    "* `torch_geometric.data.Dataset.processed_file_names()`: A list of files in the `processed_dir` which needs to be found in order to skip the processing.\n",
    "* `torch_geometric.data.Dataset.download()`: Downloads raw data into `raw_dir`.\n",
    "* `torch_geometric.data.Dataset.process()`: Processes raw data and saves it into `processed_dir`.\n",
    "\n",
    "To create a `torch_geometric.data.Dataset`, we also need to implement the above four methods. Additionally, we need to implement the following two methods: \n",
    "* `torch_geometric.data.Dataset.len()`: Returns the number of examples in the dataset.\n",
    "* `torch_geometric.data.Dataset.get()`: Implements the logic to load a single path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "In order not to have to worry about CPU memory issues, we decide to use the `torch_geometric.data.Dataset` class. \n",
    "\n",
    "First, the class is initialized with the `__init__()` method. We do not pass a `transform`, `pre_transform` or `pre_filter` function, since data augmentation, heavy computation and the restriction of data objects to a specific class are not relevant in our setting.\n",
    "\n",
    "Afterwards, the four fundamental methods `raw_file_names()`, `processed_file_names()`, `download()` and `process()` are implemented, of which the `process()` method is the most extensive. It loads the three data sources of interest: the data frame containing the node features, the data frame containing the edge features and the list containing all the links. Then, these are processed, yielding the node features, edge features, all the links and the node labels. For the `process()` method to perform its work, four other methods have been implemented: `_get_node_features()`, `_get_edge_features()`, `_get_adjacency_info()` and `_get_labels()`. They are mainly used to transform the node features, edge features, all links and node labels from the data frames to a torch tensor format that PyTorch Geometric can handle. For more information on data handling of graphs in PyTorch Geometric, click [here](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs).\n",
    "\n",
    "Lastly, the two methods `len()` and `get()` are implemented.\n",
    "\n",
    "We call our dataset `LinkPredictionDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinkPredictionDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(LinkPredictionDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['node_features.pkl', 'edge_features.pkl', 'all_links.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'not_implemented.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        #for raw_path in self.raw_paths:\n",
    "        with open(self.raw_paths[0], 'rb') as fh:\n",
    "            node_features = pickle.load(fh)\n",
    "        with open(self.raw_paths[1], 'rb') as fh:\n",
    "            edge_features = pickle.load(fh)\n",
    "        with open(self.raw_paths[2], 'r') as f:\n",
    "            all_links = json.loads(f.read())\n",
    "        \n",
    "        # Get node features\n",
    "        node_feats = self._get_node_features(node_features)\n",
    "        # Get edge features\n",
    "        edge_feats = self._get_edge_features(edge_features)\n",
    "        # Get adjacency info\n",
    "        edge_index = self._get_adjacency_info(all_links)\n",
    "        # Get labels info\n",
    "        # not required for LinkPrediction\n",
    "            \n",
    "        data = Data(x = node_feats, \n",
    "                    edge_index = edge_index, \n",
    "                    edge_attr = edge_feats#,\n",
    "                     #y = label\n",
    "                    )\n",
    "\n",
    "        torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        idx += 1\n",
    "    \n",
    "    def _get_node_features(self, node_features):\n",
    "        return torch.tensor(node_features.values, dtype=torch.float)\n",
    "        \n",
    "    def _get_edge_features(self, edge_features):\n",
    "        edge_features = edge_features.filter(items=['paper_link', 'journal_link', 'hospital_link'])\n",
    "        edge_features = pd.concat([edge_features, edge_features], ignore_index=True)\n",
    "        return torch.tensor(edge_features.values, dtype=torch.float)\n",
    "    \n",
    "    def _get_adjacency_info(self, all_links):\n",
    "        # double links:\n",
    "        for i in range(len(all_links)):\n",
    "            all_links += [reverse(all_links[i])]\n",
    "        return torch.tensor(all_links, dtype=torch.long).t().contiguous()\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our GNN applications, we need to split the edges into positive and negative training, validation and test edges.\n",
    "\n",
    "We use the [`RandomNodeSplit`](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.RandomNodeSplit) class to do this, which can be called when loading our `LinkPredictionDataset`. We need to specify the arguments `num_val` and `num_test`, which specify the share of validation and test edges in all edges. In addition, we also specify with setting `is_undirected` to True that our graph is undirected. By setting `add_negative_train_samples` to True, we add negative training samples for link prediction. When the argument `split_labels` is set to True - as in our case, then positive and negative labels are saved in distinct attributes. \n",
    "\n",
    "We aim to have a ratio of 0.1 of edges in the validation set and a ratio of 0.2 of edges in the test set. The majority of edges therefore remain in the training set.\n",
    "\n",
    "We now look at an exemplary split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "torch_geometric.seed_everything(12345) \n",
    "\n",
    "dataset = LinkPredictionDataset(root='data/', transform=RandomLinkSplit(num_val=0.1, num_test=0.3, is_undirected=True, split_labels=True, add_negative_train_samples=True))\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinkPredictionDataset(18)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Data(x=[229, 68], edge_index=[2, 6986], edge_attr=[6986, 3], pos_edge_label=[3493], pos_edge_label_index=[2, 3493], neg_edge_label=[3493], neg_edge_label_index=[2, 3493]),\n",
       " Data(x=[229, 68], edge_index=[2, 6986], edge_attr=[6986, 3], pos_edge_label=[582], pos_edge_label_index=[2, 582], neg_edge_label=[582], neg_edge_label_index=[2, 582]),\n",
       " Data(x=[229, 68], edge_index=[2, 8150], edge_attr=[8150, 3], pos_edge_label=[1746], pos_edge_label_index=[2, 1746], neg_edge_label=[1746], neg_edge_label_index=[2, 1746]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a dataset, where the node feature matrix `x` is of shape `[229, 68]`, which indicates that we have 229 nodes, each having 68 features. \n",
    "\n",
    "The positive and negative training edges are given by the edges contained in `pos_edge_label_index` and `neg_edge_label_index` with shape `[2, 3493]`, respectively. \n",
    "Similarly, the positive and negative validation edges are contained in `pos_edge_label_index` and `neg_edge_label_index` with shape `[2, 582]`.\n",
    "Finally, `pos_edge_label_index` and `neg_edge_label_index` with shape `[2, 1746]` give the positive and negative test edges.\n",
    "\n",
    "Note that `pos_edge_label` is a tensor containing only ones, indicating that the edges in `pos_edge_label_index` exist, while `neg_edge_label` is a tensor with only zeroes, showing that the edges `negative_edge_label_index` contains do not exist.\n",
    "\n",
    "We now see what each part of the dataset looks like for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.0976, 0.0702,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.4000, 0.2439, 0.2073,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.2000, 0.0244, 0.0616,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.2000, 0.0976, 0.1322,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.2000, 0.1220, 0.1064,  ..., 0.0000, 1.0000, 0.0000]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 55, 189,  42,  ..., 122,  97, 109],\n",
       "        [111, 210, 137,  ...,  61,  59, 100]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [0., 0., 1.],\n",
       "        [0., 1., 0.],\n",
       "        ...,\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.],\n",
       "        [0., 1., 0.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].edge_attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1.,  ..., 1., 1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].pos_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 55, 189,  42,  ...,  61,  59, 100],\n",
       "        [111, 210, 137,  ..., 122,  97, 109]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].pos_edge_label_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].neg_edge_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[180, 160,  17,  ..., 103, 150,  46],\n",
       "        [203, 176, 225,  ...,  95,  76,  51]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].neg_edge_label_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we use the `LinkPredictionDataset` only for our GNN applications of link prediction. We use different splits into training set, validation set and test set based on the respective seed we use. Of course, the sizes of the training set, validation set and test set always remain unchanged. \n",
    "\n",
    "Our GNN applications are:\n",
    "* graph autoencoder, implemented in `gae_model_gcn_encoder.ipynb`,\n",
    "* variational graph autoencoder, implemented in `vgae_model_vgcn_encoder.ipynb`,\n",
    "* SEAL, implemented in `seal_model.ipynb`.\n",
    "\n",
    "The results are analyzed and visualized in `visualization_results.ipynb` - together with the baseline results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
