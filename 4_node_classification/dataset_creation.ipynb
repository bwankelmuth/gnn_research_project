{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset creation for semi-supervised node classification\n",
    "\n",
    "In this notebook, we create the dataset required for our semi-supervised node classification task. We use this dataset for all semi-supervised node classification models, i.e., for baselines and GNNs alike."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pickle\n",
    "\n",
    "import json\n",
    "import os.path as osp\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.data import Dataset, download_url\n",
    "from torch_geometric.transforms import NormalizeFeatures, RandomNodeSplit\n",
    "\n",
    "#pd.set_option('display.max_rows', None)\n",
    "#pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse(tuples):\n",
    "    \"\"\"\n",
    "    Reverse a 2-tuple.\n",
    "    \"\"\"\n",
    "    new_tup = tuples[::-1]\n",
    "    \n",
    "    return new_tup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semi-supervised node classification\n",
    "\n",
    "Semi-supervised node classification is about classifying nodes in a graph. The term *semi-supervised* (<span style=\"font-variant:small-caps;\">Yang et al. (2016)</span>) is used due to the atypical nature of node classification. Because when training node classification models, we can usually access the full graph, including all the unlabeled nodes. We are only missing the labels of the test nodes. However, we can still use information about the test nodes, e.g., knowledge of their neighborhood in the graph, to improve the model during training. This is a significant deviation from the standard supervised setting, where unlabeled datapoints are completely unobserved during training (<span style=\"font-variant:small-caps;\">Hamilton (2020)</span>).\n",
    "\n",
    "In our task, we follow the set-up of the paper *Semi-supervised classification with graph convolutional networks* (<span style=\"font-variant:small-caps;\">Kipf and Welling (2017)</span>), where labels are only available for a small subset of nodes. \n",
    "\n",
    "We choose the variable `segment` for node classification, which has the four classes `S1`, `S2`, `S3`, `S4`, where each class features sufficiently often, so that class imbalance is no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Absolute frequencies for 'segment':\n",
      "S3    93\n",
      "S4    66\n",
      "S2    54\n",
      "S1    16\n",
      "Name: segment, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Read in data frame\n",
    "targets = pd.read_pickle(\"../1_data_processing/processed_data/targets.pkl\")\n",
    "# Print absolute frequencies\n",
    "print(f\"Absolute frequencies for 'segment':\\n{targets.segment.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch Geometric functionality for dataset creation\n",
    "\n",
    "We use the built-in dataset creation provided in PyTorch Geometric. For more information on dataset creation in PyTorch Geometric, click [here](https://pytorch-geometric.readthedocs.io/en/latest/notes/create_dataset.html).\n",
    "\n",
    "PyTorch Geometric provides two abstract classes for datasets: [`torch_geometric.data.Dataset`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.Dataset) and [`torch_geometric.data.InMemoryDataset`](https://pytorch-geometric.readthedocs.io/en/latest/modules/data.html#torch_geometric.data.InMemoryDataset). Note that `torch_geometric.data.InMemoryDataset` inherits from `torch_geometric.data.Dataset` and should be used if the whole dataset fits into CPU memory.\n",
    "\n",
    "Each dataset is passed a root folder, indicating where the datasets should be stored. This root folder is split up into two folders: the `raw_dir`, where the dataset is downloaded to, and the `processed_dir`, where the processed dataset is saved.\n",
    "\n",
    "Additionally, each dataset can be passed a `transform`, a `pre_transform` and a `pre_filter` function, all being `None` by default. The `transform` function dynamically transforms the data object before accessing, therefore being particularly useful for data augmentation. The `pre_transform` function applies the transformation before saving the data objects to disk, therefore being particularly useful for heavy precomputation which needs to be done only once. The `pre_filter` function can manually filter out data objects before saving, therefore being particularly useful for use cases which involve the restriction of data objects being of a specific class.\n",
    "\n",
    "To create a `torch_geometric.data.InMemoryDataset`, we need to implement four fundamental methods:\n",
    "* `torch_geometric.data.Dataset.raw_file_names()`: A list of files in the `raw_dir` which needs to be found in order to skip the download.\n",
    "* `torch_geometric.data.Dataset.processed_file_names()`: A list of files in the `processed_dir` which needs to be found in order to skip the processing.\n",
    "* `torch_geometric.data.Dataset.download()`: Downloads raw data into `raw_dir`.\n",
    "* `torch_geometric.data.Dataset.process()`: Processes raw data and saves it into `processed_dir`.\n",
    "\n",
    "To create a `torch_geometric.data.Dataset`, we also need to implement the above four methods. Additionally, we need to implement the following two methods: \n",
    "* `torch_geometric.data.Dataset.len()`: Returns the number of examples in the dataset.\n",
    "* `torch_geometric.data.Dataset.get()`: Implements the logic to load a single path."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our dataset\n",
    "\n",
    "In order not to have to worry about CPU memory issues, we decide to use the `torch_geometric.data.Dataset` class. \n",
    "\n",
    "First, the class is initialized with the `__init__()` method. We do not pass a `transform`, `pre_transform` or `pre_filter` function, since data augmentation, heavy computation and the restriction of data objects to a specific class are not relevant in our setting.\n",
    "\n",
    "Afterwards, the four fundamental methods `raw_file_names()`, `processed_file_names()`, `download()` and `process()` are implemented, of which the `process()` method is the most extensive. It loads the three data sources of interest: the data frame containing the node features, the data frame containing the edge features and the list containing all the links. Then, these are processed, yielding the node features, edge features, all the links and the node labels. For the `process()` method to perform its work, four other methods have been implemented: `_get_node_features()`, `_get_edge_features()`, `_get_adjacency_info()` and `_get_labels()`. They are mainly used to transform the node features, edge features, all links and node labels from the data frames to a torch tensor format that PyTorch Geometric can handle. For more information on data handling of graphs in PyTorch Geometric, click [here](https://pytorch-geometric.readthedocs.io/en/latest/notes/introduction.html#data-handling-of-graphs).\n",
    "\n",
    "Lastly, the two methods `len()` and `get()` are implemented.\n",
    "\n",
    "We call our dataset `NodeClassificationDataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeClassificationDataset(Dataset):\n",
    "    def __init__(self, root, transform=None, pre_transform=None, pre_filter=None):\n",
    "        super(NodeClassificationDataset, self).__init__(root, transform, pre_transform, pre_filter)\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return ['node_features.pkl', 'edge_features.pkl', 'all_links.txt']\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return 'not_implemented.pt'\n",
    "\n",
    "    def download(self):\n",
    "        pass\n",
    "\n",
    "    def process(self):\n",
    "        idx = 0\n",
    "        \n",
    "        with open(self.raw_paths[0], 'rb') as fh:\n",
    "            node_features = pickle.load(fh)\n",
    "        with open(self.raw_paths[1], 'rb') as fh:\n",
    "            edge_features = pickle.load(fh)\n",
    "        with open(self.raw_paths[2], 'r') as f:\n",
    "            all_links = json.loads(f.read())\n",
    "        \n",
    "        # Get node features\n",
    "        node_feats = self._get_node_features(node_features)\n",
    "        # Get edge features\n",
    "        edge_feats = self._get_edge_features(edge_features)\n",
    "        # Get adjacency info\n",
    "        edge_index = self._get_adjacency_info(all_links)\n",
    "        # Get labels info\n",
    "        label = self._get_labels(node_features)\n",
    "            \n",
    "        data = Data(x = node_feats, \n",
    "                    edge_index = edge_index, \n",
    "                    #edge_attr = edge_feats,\n",
    "                    y = label\n",
    "                    )\n",
    "\n",
    "        torch.save(data, osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        idx += 1\n",
    "        \n",
    "        self.num_classes = len(label.unique()) \n",
    "        \n",
    "    def _get_node_features(self, node_features):\n",
    "        node_features = node_features.drop(columns=['S1', 'S2', 'S3', 'S4'])\n",
    "        return torch.tensor(node_features.values, dtype=torch.float32)\n",
    "        \n",
    "    def _get_edge_features(self, edge_features):\n",
    "        edge_features = edge_features.filter(items=['paper_link', 'journal_link', 'hospital_link'])\n",
    "        edge_features = pd.concat([edge_features, edge_features], ignore_index=True)\n",
    "        return torch.tensor(edge_features.values, dtype=torch.float)\n",
    "    \n",
    "    def _get_adjacency_info(self, all_links):\n",
    "        # double links:\n",
    "        for i in range(len(all_links)):\n",
    "            all_links += [reverse(all_links[i])]\n",
    "        return torch.tensor(all_links, dtype=torch.int64).t().contiguous()\n",
    "    \n",
    "    def _get_labels(self, node_features):\n",
    "        label = node_features.filter(items=['S1', 'S2', 'S3', 'S4'])\n",
    "        return torch.tensor(label.values, dtype=torch.int64).argmax(-1)\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, f'data_{idx}.pt'))\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our applications, be it baselines or GNNs, we need to split the nodes into training set, validation set and test set.\n",
    "\n",
    "We use the [`RandomNodeSplit`](https://pytorch-geometric.readthedocs.io/en/latest/modules/transforms.html#torch_geometric.transforms.RandomNodeSplit) class to do this, which can be called when loading our `NodeClassificationDataset`. We need to specify the argument `split`, where `\"random\"` specifies that training, validation and test sets are randomly generated, according to `num_train_per_class`, `num_val` and `num_test` (as in the *Semi-supervised classification with graph convolutional networks* paper that we stick to). Here, `num_train_per_class` is the number of training samples per class, `num_val` is the number of validation samples and `num_test` is the number of test samples.\n",
    "\n",
    "As we follow the set-up of the *Semi-supervised classification with graph convolutional networks* paper, we would like to have only a small number of labeled nodes. In the datasets used in that paper, this corresponds to label rates of at most 5.2%. However, these datasets also have a very large number of nodes, the smallest being 2,708 (in the case of a label rate of 5.2%). We have a total of 229 nodes in our graph, which is considerably smaller. In order to have a sufficiently high number of nodes in the training set to enable our models to be able to generalize to unlabeled nodes, we choose to have 40 nodes in the training set, 10 of which come for each class, thereby avoiding class imbalance. For the remaining 189 nodes, we aim to have a rough 1:2 split between validation and test set. We therefore opt for 60 nodes in the validation set and for the remaining 129 nodes in the test set. Note that the standard benchmark dataset for semi-supervised node classification is \"Cora\", which can also be found in PyTorch Geometric as part of the [`Planetoid`](https://pytorch-geometric.readthedocs.io/en/latest/modules/datasets.html#torch_geometric.datasets.Planetoid) dataset. It has a training set of size 140, a validation set of size 500 and a test set of size 1000. We are guided by the 1:2 split between validation and test set.\n",
    "\n",
    "We now look at an exemplary split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "torch_geometric.seed_everything(12345) \n",
    "\n",
    "# random as in paper by Kipf and Welling\n",
    "dataset = NodeClassificationDataset(root='data/', transform=RandomNodeSplit(split=\"random\", num_train_per_class = 10, num_val = 60, num_test = 129))\n",
    "\n",
    "data = dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeClassificationDataset(18)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(x=[229, 64], edge_index=[2, 11642], y=[229], train_mask=[229], val_mask=[229], test_mask=[229])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have a dataset, where the node feature matrix `x` is of shape `[229, 64]`, which indicates that we have 229 nodes, each having 64 features. \n",
    "\n",
    "The graph connectivity in `edge_index` is given in COO format with shape `[2, 11642]`, so that we have a total of 11642 links. Note that if there is an undirected link between node 1 and node 2, PyTorch Geometric requires two directed links between the two nodes: a link from node 1 to node 2 and a link from node 2 to node 1. This means that we actually only have half as many links, i.e., a total of 5821 nodes.\n",
    "\n",
    "The node labels are contained in `y` with shape `[229, 1]`, showing that each of our 229 nodes has exactly 1 label.\n",
    "\n",
    "The three masks `train_mask`, `val_mask`, `test_mask` denote which nodes to use in the training, validation and test set. Their shape is `[229, 1]`, where `True` in the 4th position of `train_mask` indicates that the 4th node is used for training and `False` in the 227th position of `test_mask` indicates that the 227th node is not used for testing.\n",
    "\n",
    "We now see what each part of the dataset looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2000, 0.0976, 0.0702,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [0.4000, 0.2439, 0.2073,  ..., 0.0000, 1.0000, 0.0000],\n",
       "        [1.0000, 1.0000, 1.0000,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        ...,\n",
       "        [0.2000, 0.0244, 0.0616,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.0976, 0.1322,  ..., 1.0000, 0.0000, 0.0000],\n",
       "        [0.2000, 0.1220, 0.1064,  ..., 0.0000, 0.0000, 0.0000]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1, 128,   1,  ..., 172, 167, 224],\n",
       "        [ 53,  53,   8,  ...,  81,  95, 223]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1, 1, 1, 1, 1, 3, 2, 3, 3, 2, 1, 0, 1, 3, 0, 0, 3, 0, 2, 3, 3, 1,\n",
       "        1, 1, 1, 2, 1, 3, 3, 2, 2, 2, 3, 3, 3, 2, 2, 2, 3, 2, 2, 1, 1, 2, 2, 2,\n",
       "        1, 1, 3, 1, 3, 0, 2, 2, 0, 1, 1, 0, 0, 1, 1, 2, 2, 2, 3, 1, 3, 3, 2, 1,\n",
       "        3, 2, 1, 2, 2, 1, 2, 1, 0, 3, 3, 2, 2, 1, 3, 3, 2, 2, 2, 2, 2, 2, 2, 1,\n",
       "        2, 3, 2, 1, 0, 2, 3, 3, 2, 2, 2, 1, 2, 1, 2, 1, 2, 0, 2, 2, 1, 0, 1, 2,\n",
       "        2, 2, 1, 2, 2, 2, 3, 0, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 2, 2,\n",
       "        1, 1, 1, 2, 2, 2, 2, 1, 2, 2, 2, 2, 2, 1, 1, 1, 2, 1, 2, 1, 2, 2, 2, 2,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 2, 1, 1, 2, 0, 3, 2, 1, 3, 3, 0, 1,\n",
       "        1, 2, 2, 2, 1, 1, 3, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False, False, False, False,  True,  True, False,\n",
       "        False, False, False, False,  True, False, False,  True, False,  True,\n",
       "         True, False, False, False,  True, False, False,  True, False,  True,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False,  True,\n",
       "        False, False, False,  True, False, False,  True, False, False,  True,\n",
       "         True, False,  True, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False, False, False, False, False, False,\n",
       "        False,  True, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False,  True,\n",
       "         True, False,  True, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False,  True, False,  True,\n",
       "        False, False, False, False,  True, False, False, False, False, False,\n",
       "         True, False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False, False, False, False, False, False, False, False,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "        False, False, False,  True,  True, False, False,  True, False, False,\n",
       "         True, False, False, False, False, False,  True, False, False, False,\n",
       "         True,  True, False, False,  True, False, False, False, False, False,\n",
       "        False, False,  True, False, False, False, False, False, False])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.train_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([False, False, False, False,  True, False, False, False, False, False,\n",
       "         True,  True, False, False, False,  True, False, False,  True, False,\n",
       "        False, False, False, False, False,  True,  True, False, False, False,\n",
       "        False,  True, False,  True, False,  True,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True, False,\n",
       "         True, False, False, False,  True, False, False,  True, False, False,\n",
       "        False, False, False, False,  True,  True,  True, False, False,  True,\n",
       "         True, False, False, False, False, False, False,  True, False, False,\n",
       "        False, False, False, False, False, False,  True, False, False, False,\n",
       "        False, False, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False, False, False,  True, False, False,  True, False,\n",
       "        False,  True,  True, False, False, False, False, False, False, False,\n",
       "         True,  True, False, False, False, False, False, False,  True, False,\n",
       "        False, False, False,  True, False, False, False, False,  True,  True,\n",
       "        False,  True, False, False, False, False,  True, False,  True, False,\n",
       "         True, False, False, False, False, False,  True, False, False, False,\n",
       "        False, False,  True, False,  True, False,  True, False, False, False,\n",
       "         True, False,  True, False, False, False, False,  True,  True,  True,\n",
       "        False,  True, False, False, False, False,  True, False,  True, False,\n",
       "         True,  True, False, False, False,  True, False, False, False, False,\n",
       "        False, False, False, False,  True, False, False, False, False,  True,\n",
       "        False, False, False,  True, False, False, False, False, False, False,\n",
       "         True, False, False, False, False,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.val_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ True,  True,  True,  True, False,  True,  True, False, False,  True,\n",
       "        False, False,  True,  True, False, False,  True, False, False, False,\n",
       "        False,  True,  True,  True, False, False, False, False,  True, False,\n",
       "         True, False,  True, False,  True, False, False,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True, False,  True, False, False,\n",
       "        False,  True,  True, False, False,  True, False, False,  True, False,\n",
       "        False,  True, False,  True, False, False, False,  True,  True, False,\n",
       "        False, False,  True,  True,  True,  True,  True, False,  True,  True,\n",
       "         True, False,  True,  True,  True, False, False,  True,  True,  True,\n",
       "         True,  True,  True, False,  True,  True,  True,  True, False, False,\n",
       "        False,  True, False,  True,  True, False, False,  True, False,  True,\n",
       "         True, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
       "        False, False,  True,  True,  True,  True,  True, False, False, False,\n",
       "         True,  True,  True, False, False,  True,  True,  True, False, False,\n",
       "        False, False,  True,  True, False,  True, False,  True, False,  True,\n",
       "        False,  True,  True,  True, False,  True, False,  True,  True,  True,\n",
       "         True,  True, False,  True, False,  True, False,  True,  True,  True,\n",
       "        False,  True, False,  True,  True,  True,  True, False, False, False,\n",
       "         True, False,  True, False,  True,  True, False,  True, False,  True,\n",
       "        False, False,  True, False, False, False,  True, False,  True,  True,\n",
       "        False,  True,  True,  True, False,  True, False,  True,  True, False,\n",
       "        False, False,  True, False, False,  True,  True,  True,  True,  True,\n",
       "        False,  True, False,  True,  True, False, False, False, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.test_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we can also find out which labels are in the training, validation and test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 2, 1, 0, 0, 2, 1, 2, 3, 2, 1, 0, 0, 0, 0, 1, 1, 3, 1, 2, 1, 0, 3, 2,\n",
       "        0, 0, 2, 2, 1, 2, 3, 3, 3, 3, 3, 1, 2, 1, 0, 3])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[data.train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 3, 2, 3, 3, 1, 1, 2, 2, 3, 3, 1, 3, 2, 1, 2, 2, 3, 3, 2, 1, 3, 2, 2,\n",
       "        2, 1, 2, 2, 2, 2, 2, 2, 2, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 2, 3, 3, 1, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[data.val_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 1, 1, 1, 1, 3, 1, 0, 0, 3, 3, 1, 1, 3, 2, 3, 2, 2, 2, 3, 2, 2, 1,\n",
       "        1, 2, 2, 1, 3, 2, 1, 1, 2, 1, 3, 3, 2, 1, 2, 2, 2, 1, 0, 3, 2, 2, 3, 2,\n",
       "        2, 2, 2, 2, 2, 1, 2, 3, 2, 3, 2, 1, 1, 2, 0, 2, 2, 1, 0, 1, 2, 1, 2, 2,\n",
       "        2, 3, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 1, 1, 2,\n",
       "        2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 1, 1, 2, 0,\n",
       "        3, 1, 1, 2, 2, 2, 1, 2, 2])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.y[data.test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of training set: 40\n",
      "Length of validation set: 60\n",
      "Length of test set: 129\n"
     ]
    }
   ],
   "source": [
    "print(f\"Length of training set: {len(data.y[data.train_mask])}\")\n",
    "print(f\"Length of validation set: {len(data.y[data.val_mask])}\")\n",
    "print(f\"Length of test set: {len(data.y[data.test_mask])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the `NodeClassificationDataset` for all of our following applications of node classification. However, we use different splits into training set, validation set and test set based on the respective seed we use. Of course, the size of the training set, validation set and test set always remain unchanged. \n",
    "\n",
    "Our applications are:\n",
    "\n",
    "* logistic regression, implemented in `logistic_regression_baseline.ipynb`,\n",
    "* random forest classifier, implemented in `random_forest_classifier_baseline.ipynb`,\n",
    "* support vector classifier, implemented in `support_vector_classifier_baseline.ipynb`,\n",
    "* multi-layer perceptron, implemented in `multi_layer_perceptron_baseline.ipynb`,\n",
    "* graph convolutional network, implemented in `graph_convolutional_network.ipynb`,\n",
    "* GNN with Chebyshev graph spectral convolutional operator, implemented in `gnn_with_chebyshev_convolution.ipynb`,\n",
    "* graph attention network, implemented in `graph_attention_network.ipynb`.\n",
    "\n",
    "The results are analyzed and visualized in `visualization_results.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
